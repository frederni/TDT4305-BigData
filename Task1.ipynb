{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"324eb902-51f2-44b0-8ff1-730efac9900c","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[1]: True"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[1]: True","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n","dbutils.fs.rm(\"/user\", recurse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"60b530d6-b580-4de2-affb-aad878f4da86","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\n","You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"\u001b[33mWARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n","!pip install -q ipython_unittest"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"023f4d10-e729-450c-8d1a-50d494b488d6","showTitle":false,"title":""}},"outputs":[],"source":["# Loading PySpark modules that we need\n","import unittest\n","from collections import Counter\n","from pyspark.sql import DataFrame\n","from pyspark.sql.types import *"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fcda919c-b51e-4b61-9d61-80cc24f2d15e","showTitle":false,"title":""}},"source":["#### Subtask 1: defining the schema for the data\n","Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ab8bdba5-f6d6-43bf-bed9-763d99cfcc91","showTitle":false,"title":""}},"outputs":[],"source":["# Defining a schema for 'badges' table\n","badges_schema = StructType([StructField('UserId', IntegerType(), False),\n","                            StructField('Name', StringType(), False),\n","                            StructField('Date', TimestampType(), False),\n","                            StructField('Class', IntegerType(), False)])\n","\n","# Defining a schema for 'posts' table\n","posts_schema = StructType([StructField('Id', IntegerType(), False),\n","                           StructField('ParentId', IntegerType(), True),\n","                           StructField('PostTypeId', IntegerType(), False),\n","                           StructField('CreationDate', TimestampType(), False),\n","                            StructField('Score', IntegerType(), False),\n","                            StructField('ViewCount', IntegerType(), False),\n","                            StructField('Body', StringType(), False),\n","                            StructField('OwnerUserId', IntegerType(), False),\n","                            StructField('LastActivityDate', TimestampType(), False),\n","                            StructField('Title', StringType(), True),\n","                            StructField('Tags', StringType(), True),\n","                            StructField('AnswerCount', IntegerType(), False),\n","                            StructField('CommentCount', IntegerType(), False),\n","                            StructField('FavoriteCount', IntegerType(), False),\n","                            StructField('Closedate', TimestampType(), True)])\n","## YOUR IMPLEMENTATION ##\n","\n","# Defining a schema for 'users' table\n","users_schema = StructType([StructField('Id', IntegerType(), False),\n","                           StructField('Reputation', IntegerType(), False),\n","                           StructField('CreationDate', TimestampType(), False),\n","                           StructField('DisplayName', StringType(), False),\n","                           StructField('LastAccessDate', TimestampType(), False),\n","                           StructField('AboutMe', StringType(), True),\n","                           StructField('Views', IntegerType(), False),\n","                           StructField('UpVotes', IntegerType(), False),\n","                           StructField('Downvotes', IntegerType(), False)])\n","## YOUR IMPLEMENTATION ##\n","\n","# Defining a schema for 'comments' table\n","comments_schema = StructType([StructField('PostId', IntegerType(), False), \n","                              StructField('Score', IntegerType(), False), \n","                              StructField('Text', StringType(), False), \n","                              StructField('CreationDate', TimestampType(), False), \n","                              StructField('UserId', IntegerType(), False)])\n","## YOUR IMPLEMENTATION ##"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf","showTitle":false,"title":""}},"source":["#### Subtask 2: implementing two helper functions\n","Next, we need to implement two helper functions:\n","1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n","2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n","\n","Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n","\n","BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"089f87ff-f2c8-4ac8-8449-cec251c502f6","showTitle":false,"title":""}},"outputs":[],"source":["def load_csv(source_file: \"path for the CSV file to load\", schema: \"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n","  # Since header is explicit from schema, we actively remove them from our imports\n","  return (spark.read\n","         .option(\"delimiter\", '\\t')\n","         .option(\"header\", True)\n","         .schema(schema)\n","         .csv(source_file))\n","\n","def save_df(df: \"DataFrame to be saved\", table_name: \"name under which the DataFrame will be saved\") -> None:\n","  df.write.format(\"parquet\").save(\"/user/hive/warehouse/\" + table_name)\n","  #df.write.csv(\"/user/hive/warehouse/\" + table_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"39bc683c-b37a-4842-8bf8-004620b17cca","showTitle":false,"title":""}},"outputs":[],"source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n","%load_ext ipython_unittest"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"8903e903-7e4f-4c15-99bd-c9129a601fde","showTitle":false,"title":""}},"source":["#### Subtask 3: validating the implementation by running the tests\n","\n","Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n","\n","Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cd470d59-2571-4b7d-b022-9ee8f7c3e281","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Success.....\n","----------------------------------------------------------------------\n","Ran 5 tests in 57.880s\n","\n","OK\n","Out[7]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Success.....\n----------------------------------------------------------------------\nRan 5 tests in 57.880s\n\nOK\nOut[7]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["%%unittest_main\n","class TestTask1(unittest.TestCase):\n","   \n","    # test 1\n","    def test_load_badges(self):\n","        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n","        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 2\n","    def test_load_posts(self):\n","        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n","        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n","                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n","                                    'CloseDate']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 3\n","    def test_load_comments(self):\n","        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n","        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 4\n","    def test_load_users(self):\n","        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n","        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n","                                    'Views', 'UpVotes', 'DownVotes']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    # test 5\n","    def test_save_dfs(self):\n","        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n","               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n","               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n","               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n","               ]\n","\n","        for i in dfs:\n","            df = load_csv(source_file=i[0], schema=i[1])\n","            save_df(df, i[2])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0f99b257-8618-4796-aeb0-d9446863c259","showTitle":false,"title":""}},"source":["#### Subtask 4: answering to questions about Spark related concepts\n","\n","Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n","\n","1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n","\n","Write your descriptions in the next cell."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"91ec9fda-7848-4f01-8a36-3b82b78be007","showTitle":false,"title":""}},"source":["**Spark Application** is, with the lack of a better term the core of Spark and describes the how the Driver process and Worker processes interact. The Worker processes, or excecutors, are nodes responsible for the actual computations. They are assigned a specific task that they must carry out and return the result back to the Driver. Note that the Workers can also cache, or presist a query result. The Driver process is essentially the `main()`-method of our application. The driver, based on a user's code or input, creates a plan consisting of what operations that has to be excecuted and schedules it to the workers/executors. \n","\n","**SparkSession** is also a building block of using Spark, which is called by the Spark Application. A SparkSession is necessary in order to work with the Spark objects `DataFrame`, `RDD` and `Dataset`. SparkSession was implemented in Spark 2, partially replacing the elder SparkContext class\n","\n","***Transformations*** are one of the two main operations you can do with data in Spark. Transformations will change, or *transform* the data into something else based on some criteria, condition or function. A log-scaling of stock price data can be an example of a trasnformation, but filterings are also a type of transformation, so it does not have to be bijective. \n","\n","***Action*** is the other operation you can do on data, which evaluates to a specific result. Examples of actions can be `count`, `max`, `min`, `first`, `last`, `average` and so on. They often aggregate over the data to produce a certain result, and not necessarily preform any manipulations to the data. \n","\n","***Lazy Evaluation*** is an important concept in regards to the efficiency of Spark. This entails that the data will not be transformed before an action is actually called. Instead, Spark stores the call stack. Once the user needs the result from the transformation, i.e. the user has called an action on a transformed RDD, the transformation is preformed."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Task1","notebookOrigID":3187623329893330,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
